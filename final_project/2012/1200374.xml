<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Hybrid 4-Dimensional Augmented Reality Environments for Ubiquitous Markerless Context-Aware AEC/FM Applications</AwardTitle>
    <AwardEffectiveDate>08/15/2012</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2015</AwardExpirationDate>
    <AwardAmount>299961</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>07030000</Code>
      <Directorate>
        <LongName>Directorate for Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Civil, Mechanical, and Manufacturing Innovation</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Dennis Wenger</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>The objective of this project is to test whether a framework proposed by the PIs can in near real-time read, write, and receive feedback from a model which fuses pictures from mobile devices and Building Information Models for the purpose of providing ubiquitous and marker-less contextual awareness for Architecture/ Engineering/ Construction and Facility Management (AEC/FM) applications. According to the framework, field personnel can use mobile devices to take pictures that include specific project elements (e.g., column), touch or click on the elements in the image, and be presented with (or be able to add) a detailed list of information, such as architectural/structural plan related to the physical elements. The mobile device can use onboard GPS and other sensors to perform a rough calculation of the device's field-of-view and location. Initial image processing is done on the mobile device to extract and send feature points/descriptors, field-of-view, and location to the Hybrid 4-dimensional Augmented Reality (HD4AR) server. Based on a new computer vision method, the server uses this information from the phone to derive the mobile device's position at a resolution that is an order of magnitude more accurate than with current approaches based solely on GPS. The server uses the derived high-precision camera position to determine what cyber-information is in view of the device's camera. The extracted information, along with pixel coordinates of where each cyber-information item should appear in the photo, is returned to the mobile device and visualized in augmented reality format. &lt;br/&gt;&lt;br/&gt;If successful, the results of this research will provide the first feasible platform for context aware applications which does not require reliable and high accurate GPS/sensor-based location and orientation tracking and works based on existing image collections. It further assists field personnel through visualization of queried plan and actual site information in form of augmented reality, and supports interactions among project personnel and field information. By providing immediate access to information, the proposed framework automatically provides inexpensive, global and frequent reports from the field activities, and in turn can reduce downtime, rework, waste, and ultimately cost overrun. This project also involves educational and outreach activities to promote teaching and learning, engage undergraduate and graduate students, and reach out to underrepresented groups, K-12 students, and industry professionals. These activities include development of two course modules of "visual sensing for civil infrastructure engineering and management" and "mobile cyber-physical systems," as well as creating new software tools and hands-on outreach materials for context aware AEC/FM applications, which will be widely distributed among research and professional communities.</AbstractNarration>
    <MinAmdLetterDate>07/14/2012</MinAmdLetterDate>
    <MaxAmdLetterDate>07/14/2012</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1200374</AwardID>
    <Investigator>
      <FirstName>Christopher</FirstName>
      <LastName>White</LastName>
      <EmailAddress>julesw@vt.edu</EmailAddress>
      <StartDate>07/14/2012</StartDate>
      <EndDate/>
      <RoleCode>2</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Mani</FirstName>
      <LastName>Golparvar-Fard</LastName>
      <EmailAddress>mgolpar@illinois.edu</EmailAddress>
      <StartDate>07/14/2012</StartDate>
      <EndDate/>
      <RoleCode>1</RoleCode>
    </Investigator>
    <Institution>
      <Name>Virginia Polytechnic Institute and State University</Name>
      <CityName>BLACKSBURG</CityName>
      <ZipCode>240610001</ZipCode>
      <PhoneNumber>5402315281</PhoneNumber>
      <StreetAddress>Sponsored Programs 0170</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Virginia</StateName>
      <StateCode>VA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>1631</Code>
      <Text>CIVIL INFRASTRUCTURE SYSTEMS</Text>
    </ProgramElement>
  </Award>
</rootTag>
