<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>The Effects of Survey Presentation on Nonignorable Nonresponse and Measurement Error</AwardTitle>
    <AwardEffectiveDate>12/30/2011</AwardEffectiveDate>
    <AwardExpirationDate>05/31/2013</AwardExpirationDate>
    <AwardAmount>71269</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>04050000</Code>
      <Directorate>
        <LongName>Directorate for Social, Behavioral &amp; Economic Sciences</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Social and Economic Sciences</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Cheryl L. Eavey</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Every request to take part in a survey is framed in some way. This project consists of a set of experiments that investigate how the presentation of the survey request affects nonresponse and measurement error. The experiments are guided by a theory of survey participation (the salience-leverage theory) that claims that people decide whether to take part in a survey based on whatever aspects of the survey are made salient in the presentation of the survey request and on how they evaluate those features. Two initial experiments randomly vary the description of the topic and sponsor of the survey, with hypothesized effects both on nonresponse propensities and on reporting. In the third experiment, survey design features that can mediate or reduce the error-producing influences of the survey topic and sponsor will be examined. Thus, the project experimentally tests mechanisms producing nonresponse bias and measurement errors and, once these effects have been documented, provides guidance to the survey practitioner about how to reduce their impact.&lt;br/&gt;&lt;br/&gt;While the research is theoretically motivated and features experimental control, there are important practical implications of the work for the federal statistical agencies and the larger survey community. Sometimes estimates of key social indicators (e.g., the prevalence of rape or the frequency of defensive use of handguns) vary widely across surveys. The effects explored in this project may help explain these discrepancies. In addition, this work will a) help agencies conducting surveys anticipate when different sponsors may obtain different results, b) provide evidence about potentially harmful effects on nonresponse error and measurement error of emphasizing a single purpose of a survey, and c) produce evidence regarding design features that can reduce the effects of the presentation of the survey on nonresponse and measurement error.</AbstractNarration>
    <MinAmdLetterDate>03/28/2012</MinAmdLetterDate>
    <MaxAmdLetterDate>03/28/2012</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1225617</AwardID>
    <Investigator>
      <FirstName>Roger</FirstName>
      <LastName>Tourangeau</LastName>
      <EmailAddress>RogerTourangeau@Westat.com</EmailAddress>
      <StartDate>03/28/2012</StartDate>
      <EndDate/>
      <RoleCode>1</RoleCode>
    </Investigator>
    <Institution>
      <Name>Westat Inc</Name>
      <CityName>ROCKVILLE</CityName>
      <ZipCode>208500319</ZipCode>
      <PhoneNumber>3012511500</PhoneNumber>
      <StreetAddress>1650 RESEARCH BLVD</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Maryland</StateName>
      <StateCode>MD</StateCode>
    </Institution>
    <ProgramElement>
      <Code>1333</Code>
      <Text>METHOD, MEASURE &amp; STATS</Text>
    </ProgramElement>
  </Award>
</rootTag>
