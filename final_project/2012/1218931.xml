<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>RI: Small: Non-parametric Approximate Dynamic Programming for Continuous Domains</AwardTitle>
    <AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2015</AwardExpirationDate>
    <AwardAmount>450000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Directorate for Computer &amp; Information Science &amp; Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Todd Leen</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This project concerns a machine learning technique known as reinforcement learning, which is related to, but distinct from, the notion of reinforcement learning used in psychology. The common element is that both views study changes in behavior that result from experience. In the machine learning case, the behaviors are often decision making in dynamic environments, such as controlling a robot, a factory, inventory levels for a warehouse or even drug dosage levels. Current theoretical development in this area guarantees that optimal decisions can be made by reinforcement learning algorithms, but only under restrictive assumptions that are difficult to ensure in practice. Efforts to apply reinforcement learning to significant practical problems have enjoyed some success, but such efforts often forgo theoretical guarantees and rely upon tedious parameter adjustments by experts (human trial and error) to achieve success.&lt;br/&gt;&lt;br/&gt;This research seeks to reduce the amount of human trial and error needed to make reinforcement learning successful, thereby making it a more accessible tool to a wider range of people. Specifically, it will focus on algorithms for domains described by continuous variables, seeking to provide stronger theoretical guarantees for such domains as well as an approach that balances the anticipated benefit of trying new things with the benefit of sticking to what is already known about a problem (exploration vs. exploitation). A practical benefit of success in this area would be improved techniques that make it easier for people to deploy algorithms that learn and improve performance in a variety of practical tasks like those mentioned above: robot or factory control, inventory management, or drug delivery.&lt;br/&gt;&lt;br/&gt;This project plans to use a model helicopter as a challenge domain, but it is not about helicopter control per se. Rather, it seeks to develop general techniques that can apply to many problems, including helicopters, and will use model helicopters as an inexpensive and fun way to motivate students. The project aims to develop a model helicopter simulator (to reduce the cost and risk of trying everything on an actual helicopter) and plans to make this simulator available to the research community, providing a fun and challenging benchmark problem.</AbstractNarration>
    <MinAmdLetterDate>07/24/2012</MinAmdLetterDate>
    <MaxAmdLetterDate>07/24/2012</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1218931</AwardID>
    <Investigator>
      <FirstName>Ronald</FirstName>
      <LastName>Parr</LastName>
      <EmailAddress>parr@cs.duke.edu</EmailAddress>
      <StartDate>07/24/2012</StartDate>
      <EndDate/>
      <RoleCode>1</RoleCode>
    </Investigator>
    <Institution>
      <Name>Duke University</Name>
      <CityName>Durham</CityName>
      <ZipCode>277054010</ZipCode>
      <PhoneNumber>9196843030</PhoneNumber>
      <StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>North Carolina</StateName>
      <StateCode>NC</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramElement>
  </Award>
</rootTag>
