<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>SHF: Small:Scalable Support for Concurrency in Multicore Systems</AwardTitle>
    <AwardEffectiveDate>07/01/2012</AwardEffectiveDate>
    <AwardExpirationDate>06/30/2015</AwardExpirationDate>
    <AwardAmount>399997</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05010000</Code>
      <Directorate>
        <LongName>Directorate for Computer &amp; Information Science &amp; Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Computer and Communication Foundations</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Hong Jiang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Processor designs are predicted to continue the many-core trend, often with heterogeneous computational components. While the raw compute power may increase roughly linearly with the core count, unfortunately, realizing the available computational power at the application level remains a challenge. The gap between CPU and memory speeds continues to widen, resulting in the memory system often being unable to feed the computational demands. Parallel application developers and users alike must be aware of the details of the underlying hardware and runtime details in order to extract the most benefit from the system, compromising performance portability. Programmers are also increasingly exploiting concurrency via the use of pre-parallelized libraries, resulting in poor composability. The proposed research aims to address these issues by improving the ease of use, scalability, and energy efficiency of multicore and multiprocessor systems, with impact on environments ranging from smart phone to servers.&lt;br/&gt;&lt;br/&gt;As part of this research, a "pay-as-you-use" application-adaptive approach is used to develop a novel on-chip memory system. The key observation leveraged is that high-level modular application structure has predictable spatial locality. Rather than use a rigid parameter for the cache line granularity as in current designs, the underlying cache design adapts to match existing access granularity. Additionally, this research aims to develop runtime techniques that map application tasks to hardware compute resources by a) respecting the dependencies across tasks, b) matching task needs with the computational and memory capabilities of the resource, and c) determining the appropriate degree of parallelism at each level to minimize execution time and energy consumption. The new memory system design will improve on-chip storage utilization, eliminate energy wasted in transferring unused bytes of data, and reduce the pressure on off-chip memory bandwidth. The new runtime techniques will improve the ease of use and scalability of computational utilization by the increasingly innovative applications of the future.</AbstractNarration>
    <MinAmdLetterDate>06/13/2012</MinAmdLetterDate>
    <MaxAmdLetterDate>06/13/2012</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1217920</AwardID>
    <Investigator>
      <FirstName>Sandhya</FirstName>
      <LastName>Dwarkadas</LastName>
      <EmailAddress>sandhya@cs.rochester.edu</EmailAddress>
      <StartDate>06/13/2012</StartDate>
      <EndDate/>
      <RoleCode>1</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Rochester</Name>
      <CityName>ROCHESTER</CityName>
      <ZipCode>146270140</ZipCode>
      <PhoneNumber>5852754031</PhoneNumber>
      <StreetAddress>518 HYLAN, RIVER CAMPUSBOX 27014</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>New York</StateName>
      <StateCode>NY</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7798</Code>
      <Text>SOFTWARE &amp; HARDWARE FOUNDATION</Text>
    </ProgramElement>
  </Award>
</rootTag>
